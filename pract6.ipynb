{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54deb9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sentence tokenization\n",
      "===================\n",
      " ['Natural language processing (NLP) is a machine learning technology that gives computers the ability to interpret, manipulate, and comprehend human language.']\n",
      "\n",
      "word tokenization\n",
      "===================\n",
      "\n",
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'machine', 'learning', 'technology', 'that', 'gives', 'computers', 'the', 'ability', 'to', 'interpret', ',', 'manipulate', ',', 'and', 'comprehend', 'human', 'language', '.']\n",
      "\n",
      "POS Tagging\n",
      "===========\n",
      " [[('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('machine', 'NN'), ('learning', 'VBG'), ('technology', 'NN'), ('that', 'WDT'), ('gives', 'VBZ'), ('computers', 'NNS'), ('the', 'DT'), ('ability', 'NN'), ('to', 'TO'), ('interpret', 'VB'), (',', ','), ('manipulate', 'VB'), (',', ','), ('and', 'CC'), ('comprehend', 'VBP'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')]]\n",
      "\n",
      "chunking\n",
      "========\n",
      "\n",
      "Tree:  [Tree('S', [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), Tree('ORGANIZATION', [('NLP', 'NNP')]), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('machine', 'NN'), ('learning', 'VBG'), ('technology', 'NN'), ('that', 'WDT'), ('gives', 'VBZ'), ('computers', 'NNS'), ('the', 'DT'), ('ability', 'NN'), ('to', 'TO'), ('interpret', 'VB'), (',', ','), ('manipulate', 'VB'), (',', ','), ('and', 'CC'), ('comprehend', 'VBP'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')])]\n"
     ]
    }
   ],
   "source": [
    "#6A\n",
    "''' \n",
    "Aim: Part of speech Tagging and chunking of user defined text.\n",
    "Part of speech POS: Labeling each word in a sentence with its corresponding part of speech.\n",
    "Chunking: Groups words into larger meaningful units, often referred to as chunks'''\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')\n",
    "from nltk import tokenize\n",
    "from nltk import tag\n",
    "from nltk import chunk\n",
    "para = \"Natural language processing (NLP) is a machine learning technology that gives computers the ability to interpret, manipulate, and comprehend human language.\"\n",
    "\n",
    "sents = tokenize.sent_tokenize(para)\n",
    "print(\"\\nsentence tokenization\\n===================\\n\",sents)\n",
    "\n",
    "print(\"\\nword tokenization\\n===================\\n\")\n",
    "for index in range(len(sents)):\n",
    "    words = tokenize.word_tokenize(sents[index])\n",
    "\n",
    "print(words)\n",
    "\n",
    "#POS tagging\n",
    "tagged_words = []\n",
    "for index in range(len(sents)):\n",
    "    tagged_words.append(tag.pos_tag(words))\n",
    "\n",
    "print(\"\\nPOS Tagging\\n===========\\n\",tagged_words)\n",
    "\n",
    "#Chunking\n",
    "tree = []\n",
    "for index in range(len(sents)):\n",
    "    tree.append(chunk.ne_chunk(tagged_words[index]))\n",
    "\n",
    "print(\"\\nchunking\\n========\\n\")\n",
    "print(\"Tree: \",tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff82713e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Natural language processing (NLP) is an interdisciplinary subfield of computer science and information retrieval. It is primarily concerned with giving computers the ability to support and manipulate human language. It involves processing natural language datasets, such as text corpora or speech corpora, using either rule-based or probabilistic machine learning approaches. \n",
      "\n",
      "Noun phrases: ['Natural language processing', 'NLP', 'an interdisciplinary subfield', 'computer science', 'information retrieval', 'It', 'computers', 'the ability', 'human language', 'It', 'natural language datasets', 'text corpora', 'speech corpora', 'either rule-based or probabilistic machine learning approaches']\n",
      "Verbs: ['give', 'support', 'manipulate', 'involve', 'process', 'use', 'base', 'learn']\n"
     ]
    }
   ],
   "source": [
    "#6B\n",
    "''' \n",
    "Aim: Named Entity recognition of user defined text.\n",
    "Named Entity Recognition: It identifies and classifies entities like names,\n",
    "locations, organizations, date and more in the given text'''\n",
    "\n",
    "import spacy\n",
    "#Loading English tokenizer, tagger, parser, and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Processing whole documents\n",
    "text = (\n",
    "    \"Natural language processing (NLP) is an \"\n",
    "    \"interdisciplinary subfield of computer science and information \"\n",
    "    \"retrieval. It is primarily concerned with giving computers the \"\n",
    "    \"ability to support and manipulate human language. It involves \"\n",
    "    \"processing natural language datasets, such as text corpora or \"\n",
    "    \"speech corpora, using either rule-based or probabilistic machine \"\n",
    "    \"learning approaches.\"\n",
    ")\n",
    "print(\"Original text:\",text,\"\\n\")\n",
    "\n",
    "#Processing the text with spacy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4bd8e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping help\\tagsets.zip.\n",
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First tagged sentence:\n",
      " [('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
      "\n",
      "First parsed sentence structure:\n",
      " (S\n",
      "  (NP-SBJ\n",
      "    (NP (NNP Pierre) (NNP Vinken))\n",
      "    (, ,)\n",
      "    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
      "    (, ,))\n",
      "  (VP\n",
      "    (MD will)\n",
      "    (VP\n",
      "      (VB join)\n",
      "      (NP (DT the) (NN board))\n",
      "      (PP-CLR (IN as) (NP (DT a) (JJ nonexecutive) (NN director)))\n",
      "      (NP-TMP (NNP Nov.) (CD 29))))\n",
      "  (. .))\n",
      "(S\n",
      "  (NP-SBJ\n",
      "    (NP (NNP Pierre) (NNP Vinken))\n",
      "    (, ,)\n",
      "    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
      "    (, ,))\n",
      "  (VP\n",
      "    (MD will)\n",
      "    (VP\n",
      "      (VB join)\n",
      "      (NP (DT the) (NN board))\n",
      "      (PP-CLR (IN as) (NP (DT a) (JJ nonexecutive) (NN director)))\n",
      "      (NP-TMP (NNP Nov.) (CD 29))))\n",
      "  (. .))\n",
      "                                                     S                                                                         \n",
      "                         ____________________________|_______________________________________________________________________   \n",
      "                        |                                               VP                                                   | \n",
      "                        |                        _______________________|___                                                 |  \n",
      "                      NP-SBJ                    |                           VP                                               | \n",
      "         _______________|___________________    |     ______________________|______________________________________          |  \n",
      "        |          |              ADJP      |   |    |        |                PP-CLR                              |         | \n",
      "        |          |           ____|____    |   |    |        |          ________|_________                        |         |  \n",
      "        NP         |          NP        |   |   |    |        NP        |                  NP                    NP-TMP      | \n",
      "   _____|____      |     _____|____     |   |   |    |     ___|____     |    ______________|__________        _____|_____    |  \n",
      " NNP        NNP    ,    CD        NNS   JJ  ,   MD   VB   DT       NN   IN  DT             JJ         NN    NNP          CD  . \n",
      "  |          |     |    |          |    |   |   |    |    |        |    |   |              |          |      |           |   |  \n",
      "Pierre     Vinken  ,    61       years old  ,  will join the     board  as  a         nonexecutive director Nov.         29  . \n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    }
   ],
   "source": [
    "#6C\n",
    "''' \n",
    "Aim: Named Entity recognition with diagram using NLTK corpus â€“ treebank.\n",
    "The Treebank corpus, part of the Penn\n",
    "Treebank, consists of annotated linguistic data, \n",
    "including POS tags and syntactic structures.\n",
    "\n",
    "The ne_chunk() function returns a tree structure where\n",
    "named entities are labeled into categories such as PERSON, ORGANIZATION,\n",
    "LOCATION, DATE, TIME, GPE (Geopolitical Entity), MONEY, and FACILITY'''\n",
    "\n",
    "import nltk\n",
    "nltk.download('treebank') # For parsed sentences\n",
    "nltk.download('tagsets') # For part-of-speech tags\n",
    "nltk.download('book') # For additional resources (needed for .draw())\n",
    "\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "#Accessing the first tagged sentence\n",
    "tagged_sentence = treebank.tagged_sents()[0]\n",
    "print(\"First tagged sentence:\\n\", tagged_sentence)\n",
    "\n",
    "#Accessing the first parsed sentence\n",
    "parsed_sentence = treebank.parsed_sents()[0]\n",
    "print(\"\\nFirst parsed sentence structure:\\n\", parsed_sentence)\n",
    "\n",
    "print(parsed_sentence) # or parsed_sentence.pretty_print() for a formatted output\n",
    "print(parsed_sentence.pretty_print())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
