{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff697bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This tool is an a beta stage\n",
      " Alexa developers can use Get Metrics API to seamlessly\n",
      "analyse metric\n",
      " It also supports custom skill model, prebuilt Flash Briefing model, and the Smart\n",
      "Home Skill API\n",
      " You can use this tool for creation of monitors, alarms, and dashboards that\n",
      "spotlight changes\n",
      " The release of these three tools will enable developers to create visual rich\n",
      "skills for Alexa devices with screens\n",
      " Amazon describes these tools as the collection of tech and\n",
      "tools for creating visually rich and interactive voice experiences\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#4A\n",
    "''' \n",
    "Aim: Tokenization using Pythonâ€™s split() function.\n",
    "'''\n",
    "text = \"\"\" This tool is an a beta stage. Alexa developers can use Get Metrics API to seamlessly\n",
    "analyse metric. It also supports custom skill model, prebuilt Flash Briefing model, and the Smart\n",
    "Home Skill API. You can use this tool for creation of monitors, alarms, and dashboards that\n",
    "spotlight changes. The release of these three tools will enable developers to create visual rich\n",
    "skills for Alexa devices with screens. Amazon describes these tools as the collection of tech and\n",
    "tools for creating visually rich and interactive voice experiences. \"\"\"\n",
    "data = text.split('.')\n",
    "for i in data:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9bb002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:10: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Artophilic\\AppData\\Local\\Temp\\ipykernel_22656\\1568572285.py:10: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  tk = RegexpTokenizer('\\s+', gaps=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'to', 'study', 'Natural', 'Language', 'Processing', 'in', 'Python']\n"
     ]
    }
   ],
   "source": [
    "#4b\n",
    "''' \n",
    "Aim: Tokenization using Regular Expression (RegEx).\n",
    "Tokenization using Regular Expressions (RegEx) is a more advanced technique for breaking\n",
    "down text into smaller units based on specific patterns'''\n",
    "import nltk\n",
    "# import RegexpTokenizer() method from nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Create a reference variable for Class RegexpTokenizer\n",
    "tk = RegexpTokenizer('\\s+', gaps=True)\n",
    "\n",
    "# Create a string input\n",
    "str = \"I love to study Natural Language Processing in Python\"\n",
    "# Use tokenize method\n",
    "tokens = tk.tokenize(str)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cf5c3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'to', 'study', 'Natural', 'Language', 'Processing', 'in', 'Python']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#4C\n",
    "''' \n",
    "Aim: Tokenization using NLTK.\n",
    "word_tokenize() function in NLTK uses a pre-trained model to split text into words\n",
    "'''\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Create a string input\n",
    "str = \"I love to study Natural Language Processing in Python\"\n",
    "# Download the 'punkt_tab' resource\n",
    "nltk.download('punkt_tab')\n",
    "# Use tokenize method\n",
    "print(word_tokenize(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c373f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'to', 'study', 'Natural', 'Language', 'Processing', 'in', 'Python']\n"
     ]
    }
   ],
   "source": [
    "#4D\n",
    "''' \n",
    "Aim: Tokenization using spaCy library.\n",
    "'''\n",
    "import spacy\n",
    "nlp = spacy.blank(\"en\")\n",
    "# Create a string input\n",
    "str = \"I love to study Natural Language Processing in Python\"\n",
    "# Create an instance of document;\n",
    "# doc object is a container for a sequence of Token objects.\n",
    "doc = nlp(str)\n",
    "# Read the words; Print the words\n",
    "words = [word.text for word in doc]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cc23a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'to', 'study', 'natural', 'language', 'processing', 'in', 'python']\n"
     ]
    }
   ],
   "source": [
    "#4E\n",
    "''' \n",
    "Aim: Tokenization using Keras\n",
    "'''\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "#text_to_word_sequence: Convert the string into lowercase words and split the string into words.\n",
    "str1 = \"I love to study Natural Language Processing in Python\"\n",
    "tokens = text_to_word_sequence(str1)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fc03f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Players', 'unknown', 'battlegrounds', 'ready', 'to', 'launch']\n"
     ]
    }
   ],
   "source": [
    "#4F\n",
    "''' \n",
    "Tokenization using Gensim.\n",
    "gensim.utils.simple_preprocess(): Method used for tokenization.\n",
    "'''\n",
    "from gensim.utils import tokenize\n",
    "# Create a string input\n",
    "input_str = \"Players unknown battlegrounds ready to launch\"\n",
    "tokens = list(tokenize(input_str)) # Tokenize the text\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
