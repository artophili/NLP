{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c54dd8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Book', 'that', 'flight']\n",
      "(S (VP (VP Book) (NP (Det that) (NP flight))))\n",
      "      S             \n",
      "      |              \n",
      "      VP            \n",
      "  ____|____          \n",
      " |         NP       \n",
      " |     ____|____     \n",
      " VP  Det        NP  \n",
      " |    |         |    \n",
      "Book that     flight\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#7A\n",
    "''' \n",
    "Aim: Define grammar using nltk. Analyse a sentence using the same\n",
    "\n",
    "Defining grammar: creating structured\n",
    "rules to analyze and process sentences based on grammatical principles\n",
    "\n",
    "Context free grammar (CFG) is widely used in NLTK.\n",
    "In CFG, the sentences are broken into hierarchical structures consisting\n",
    "non-terminal and terminal symbols.\n",
    "Non-terminal: It contains noun phrases, verb phrases etc\n",
    "Terminals: Actual words'''\n",
    "\n",
    "#Need to import library\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "#Defining a grammar\n",
    "#S: Start node then with the help of '->' we will define\n",
    "#the sequence of the noun, verb, determinants\n",
    "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
    "S -> VP\n",
    "VP -> VP NP\n",
    "NP -> Det NP\n",
    "Det -> 'that'\n",
    "NP -> 'flight'\n",
    "VP -> 'Book'\n",
    "\"\"\")\n",
    "\n",
    "#Input sentence\n",
    "sentence = \"Book that flight\"\n",
    "\n",
    "#Tokenizing the sentence\n",
    "all_tokens = tokenize.word_tokenize(sentence)\n",
    "print(all_tokens)\n",
    "\n",
    "#Creating a instance of the ChartParser for parsing\n",
    "parser = nltk.ChartParser(grammar1)\n",
    "\n",
    "for tree in parser.parse(all_tokens):\n",
    "    print(tree)\n",
    "    tree.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9548bb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected\n",
      "Rejected\n",
      "Accepted\n",
      "Accepted\n",
      "Rejected\n",
      "Rejected\n",
      "Rejected\n",
      "Rejected\n",
      "Accepted\n"
     ]
    }
   ],
   "source": [
    "#7B\n",
    "''' \n",
    "Aim: Accept the input string with Regular expression of FA: 101+\n",
    "In Natural Language Processing (NLP) and automata theory, Finite Automata (FA) and\n",
    "Regular Expressions (RegEx) are widely used to process and recognize patterns in strings.\n",
    "\n",
    "FA:101+ : It represents a RExp that matches strings containing the pattern \"101\" followed by 1 or more 1'''\n",
    "\n",
    "def FA(s):\n",
    "    if len(s) < 3:\n",
    "        return 'Rejected'\n",
    "    if s[0] == '1':\n",
    "        if s[1] == '0':\n",
    "            if s[2] == '1':\n",
    "                for i in range(3, len(s)):\n",
    "                    if s[i] != '1':\n",
    "                        return 'Rejected'\n",
    "                return 'Accepted'\n",
    "            return 'Rejected'\n",
    "        return 'Rejeceted'\n",
    "    return 'Rejected'\n",
    "\n",
    "inputs = ['1', '10101', '101', '10111', '01010', '100', '','10111101', '1011111']\n",
    "for i in inputs:\n",
    "    print(FA(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bea8d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted\n",
      "Accepted\n",
      "Accepted\n",
      "Rejected\n",
      "Rejected\n",
      "Rejected\n",
      "Rejected\n"
     ]
    }
   ],
   "source": [
    "#7C\n",
    "''' \n",
    "Aim: Accept the input string with Regular expression of FA: (a+b)*bba'''\n",
    "\n",
    "def FA(s):\n",
    "    size = 0\n",
    "    # scan complete string and make sure that it contains only 'a' & 'b'\n",
    "    for i in s:\n",
    "        if i == 'a' or i == 'b': # Corrected the condition\n",
    "            size += 1\n",
    "        else:\n",
    "            return 'Rejected'\n",
    "    if size >= 3:\n",
    "        if s[size - 3] == 'b':\n",
    "            if s[size - 2] == 'b':\n",
    "                if s[size - 1] == 'a':\n",
    "                    return 'Accepted'\n",
    "    return 'Rejected'\n",
    "inputs = ['bba', 'ababbba', 'abba', 'abb', 'baba', 'bbb', '']\n",
    "for i in inputs:\n",
    "    print(FA(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be28bf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['I', 'saw', 'a', 'bird', 'in', 'my', 'balcony']\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V saw) (NP (Det a) (N bird)))\n",
      "    (PP (P in) (NP (Det my) (N balcony)))))\n",
      "     S                                  \n",
      "  ___|___________                        \n",
      " |               VP                     \n",
      " |        _______|________               \n",
      " |       VP               PP            \n",
      " |    ___|___          ___|___           \n",
      " |   |       NP       |       NP        \n",
      " |   |    ___|___     |    ___|_____     \n",
      " NP  V  Det      N    P  Det        N   \n",
      " |   |   |       |    |   |         |    \n",
      " I  saw  a      bird  in  my     balcony\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#7D\n",
    "''' \n",
    "Aim: Implementation of Deductive Chart Parsing using context free grammar\n",
    "and a given sentence.\n",
    "\n",
    "Deductive Chart Parsing is a dynamic programming-based parsing technique used in\n",
    "Natural Language Processing (NLP) to analyze sentence structures using Context-Free\n",
    "Grammar (CFG).\n",
    "\n",
    "It efficiently parses complex and ambiguous sentences by incrementally\n",
    "building a parse chart and applying inference rules to recognize valid phrases.\n",
    "\n",
    "The process begins with empty parse chart\n",
    "CFG rules are applied to identify grammatical structures.\n",
    "Intermediate results are stored to avoid redundant computations.\n",
    "Then parsing continues by deducing new possible structures until the entire sentence is parsed.\n",
    "'''\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "# Define a modified context-free grammar\n",
    "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'a' | 'my'\n",
    "N -> 'bird' | 'balcony'\n",
    "V -> 'saw'\n",
    "P -> 'in'\n",
    "\"\"\")\n",
    "# Input sentence\n",
    "sentence = \"I saw a bird in my balcony\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "all_tokens = tokenize.word_tokenize(sentence)\n",
    "\n",
    "print(\"Tokens:\", all_tokens) # Output: ['I', 'saw', 'a', 'bird', 'in', 'my', 'balcony']\n",
    "\n",
    "# Create a chart parser with the defined grammar\n",
    "parser = nltk.ChartParser(grammar1)\n",
    "\n",
    "# Parse the tokenized sentence and print only one parse tree\n",
    "for tree in parser.parse(all_tokens):\n",
    "    print(tree) # Print each parse tree\n",
    "    tree.pretty_print() # Display the parse tree in a pretty format\n",
    "    break # Exit after printing the first (and only) parse tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4611754a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
