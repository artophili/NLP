{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf52dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10A\n",
    "''' \n",
    "Aim: Parse a sentence and draw a tree using malt parsing.\n",
    "Maltparser is a tool for dependency parsing.\n",
    "It generate dependency trees for languages, leveraging ML.\n",
    "\n",
    "For this project \n",
    "a. Java should be installed.\n",
    "b. maltparser-1.7.2 zip file should be copied in\n",
    "C:\\Users\\AppData\\Local\\Programs\\Python\\Python39 folder and should be\n",
    "extracted in the same folder.\n",
    "c. engmalt.linear-1.7.mco & engmalt.poly-1.7.mco file should be copied to\n",
    "C:\\Users\\ AppData\\Local\\Programs\\Python\\Python39 folder\n",
    "'''\n",
    "from nltk.parse import malt\n",
    "mp = malt.MaltParser('maltparser-1.7.2', 'engmalt.linear-1.7.mco')\n",
    "#file t = mp.parse_one('I saw a bird from my window.'.split()).tree()\n",
    "print(t)\n",
    "t.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56d9f60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:13: SyntaxWarning: invalid escape sequence '\\k'\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\k'\n",
      "C:\\Users\\Artophilic\\AppData\\Local\\Temp\\ipykernel_19252\\879525029.py:13: SyntaxWarning: invalid escape sequence '\\k'\n",
      "  s = '''Good cake cost Rs.1500\\kg in Hong Kong. Please buy me one of them.\\n\\nThanks.'''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'cake', 'cost', 'Rs.1500\\\\kg', 'in', 'Hong-Kong', '.']\n",
      "['Please', 'buy', 'me', 'one', 'of', 'them', '.']\n",
      "['Thanks', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#10B\n",
    "''' \n",
    "Aim: Multiword Expressions in NLP.\n",
    "Multiword Expressions (MWEs) are combinations of words that together convey a single\n",
    "meaning or represent a specific concept\n",
    "Eg. Strong coffee\n",
    "'''\n",
    "import nltk\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "# Download 'punkt_tab' for sentence tokenization\n",
    "nltk.download('punkt_tab')\n",
    "s = '''Good cake cost Rs.1500\\kg in Hong Kong. Please buy me one of them.\\n\\nThanks.'''\n",
    "mwe = MWETokenizer([('New', 'York'), ('Hong', 'Kong')],\n",
    "separator='-')\n",
    "for sent in sent_tokenize(s):\n",
    "    print(mwe.tokenize(word_tokenize(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26645c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reliance', 'reliance', 'reliance', 'reliance', 'mumbaihyper', 'mumbaihyper', 'mumbaihyper', 'kmtrading', 'kmtrading', 'kmtrading', 'kmtrading', 'kmtrading']\n"
     ]
    }
   ],
   "source": [
    "#10C\n",
    "''' \n",
    "Aim: Normalized Web Distance and Word Similarity. \n",
    "Normalized web distance is used to calclate the similarity or disimilarity between\n",
    "two words or concepts by utilizing the vast amount of information.\n",
    "It is based on the principle that the closer the two words are in meaning, the more likely\n",
    "it is that they will co-occur in similar context.\n",
    "\n",
    "Normalized Web Distance or text similarity, especially using \n",
    "Jaro-Winkler distance and agglomerative clustering.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import textdistance\n",
    "import sklearn\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "texts = ['Reliance supermarket', 'Reliance hypermarket','Reliance', 'Reliance','Mumbai Hyper', 'Mumbai dxb','mumbai airport','k.m trading', 'KM Trading', 'KM trade', 'K.M.Trading', 'KM.Trading']\n",
    "\n",
    "def normalize(text):\n",
    "    return re.sub('[^a-z0-9]+', '',text.lower())\n",
    "\n",
    "def group_texts(texts, threshold=0.4):\n",
    "    normalized_texts = np.array([normalize(text) for text in texts])\n",
    "    distances = 1 - np.array([[textdistance.jaro_winkler(one, another) for one in normalized_texts] for another in normalized_texts])\n",
    "\n",
    "    clustering = AgglomerativeClustering(distance_threshold=threshold,metric='precomputed',linkage=\"complete\",n_clusters=None).fit(distances)\n",
    "\n",
    "    centers = dict()\n",
    "\n",
    "    for cluster_id in set(clustering.labels_):\n",
    "        index = clustering.labels_ == cluster_id\n",
    "        centrality = distances[:, index][index].sum(axis=1)\n",
    "        centers[cluster_id] = normalized_texts[index][centrality.argmin()]\n",
    "    return [centers[i] for i in clustering.labels_]\n",
    "\n",
    "l1 = []\n",
    "for i in group_texts(texts):\n",
    "    l1.append(str(i))\n",
    "\n",
    "print(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45cc8fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Artophilic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Synset.name of Synset('bank.n.01')>: <bound method Synset.definition of Synset('bank.n.01')>\n",
      "<bound method Synset.name of Synset('set.n.01')>: <bound method Synset.definition of Synset('set.n.01')>\n",
      "<bound method Synset.name of Synset('put.v.01')>: <bound method Synset.definition of Synset('put.v.01')>\n"
     ]
    }
   ],
   "source": [
    "#10D\n",
    "''' \n",
    "Aim: Word Sense Disambiguation.\n",
    "It involves determining which sense of a word is being used in a particular context.\n",
    "'''\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "# Download the 'wordnet' dataset before using it.\n",
    "nltk.download('wordnet')\n",
    "def get_first_sense(word, pos=None):\n",
    "    if pos:\n",
    "        synsets = wn.synsets(word,pos)\n",
    "    else:\n",
    "        synsets = wn.synsets(word)\n",
    "    return synsets[0]\n",
    "\n",
    "best_synset = get_first_sense('bank')\n",
    "print ('%s: %s' % (best_synset.name, best_synset.definition))\n",
    "best_synset = get_first_sense('set','n')\n",
    "print ('%s: %s' % (best_synset.name, best_synset.definition))\n",
    "best_synset = get_first_sense('set','v')\n",
    "print ('%s: %s' % (best_synset.name, best_synset.definition))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
